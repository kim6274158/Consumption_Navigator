"""
ì¹´ë“œê³ ë¦´ë¼ ì¸ê¸°ìˆœìœ„(TOP100) ìŠ¤í¬ë˜í¼
ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë¨
JavaScript ë Œë”ë§ì„ ìœ„í•´ Playwright ì‚¬ìš©
"""

from bs4 import BeautifulSoup
import json
import time
import csv
from datetime import datetime
from typing import List, Dict, Optional
import re

try:
    from playwright.sync_api import sync_playwright, Browser, Page
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âš ï¸  Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install playwright && playwright install chromium' ì‹¤í–‰ í•„ìš”")


class CardGorillaScraper:
    def __init__(self, use_playwright: bool = True):
        self.base_url = "https://www.card-gorilla.com"
        self.use_playwright = use_playwright and PLAYWRIGHT_AVAILABLE
        self.delay = 2  # ìš”ì²­ ê°„ ì§€ì—°ì‹œê°„ (ì´ˆ)
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None

        if self.use_playwright:
            self.playwright = sync_playwright().start()
            self.browser = self.playwright.chromium.launch(headless=True)
            self.page = self.browser.new_page()
            self.page.set_viewport_size({"width": 1920, "height": 1080})

    def __del__(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.page:
            self.page.close()
        if self.browser:
            self.browser.close()
        if hasattr(self, 'playwright'):
            self.playwright.stop()

    def get_page(self, url: str) -> Optional[BeautifulSoup]:
        """ì›¹í˜ì´ì§€ë¥¼ ê°€ì ¸ì™€ì„œ íŒŒì‹± (Playwright ì‚¬ìš©)"""
        try:
            if not self.use_playwright:
                raise Exception("Playwrightê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

            time.sleep(self.delay)
            self.page.goto(url, wait_until='networkidle', timeout=30000)

            # ranking_wrapì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                self.page.wait_for_selector('.ranking_wrap', timeout=10000)
            except:
                # ì„ íƒìê°€ ì—†ì–´ë„ ê³„ì† ì§„í–‰
                pass

            # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì½˜í…ì¸  ë¡œë”©)
            time.sleep(2)

            html = self.page.content()
            return BeautifulSoup(html, 'html.parser')
        except Exception as e:
            print(f"Error fetching {url}: {e}")
            return None

    def scrape_top100_cards(self, term: str = 'weekly') -> List[Dict]:
        """
        ê³ ë¦´ë¼TOP100 í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
        ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜: /chart/top100?term=weekly

        Args:
            term: ê¸°ê°„ ì„ íƒ ('weekly', 'monthly' ë“±)
        """
        url = f"{self.base_url}/chart/top100?term={term}"
        soup = self.get_page(url)

        if not soup:
            return []

        cards = []

        # ranking_wrap ì˜ì—­ ì°¾ê¸°
        ranking_section = soup.find('div', class_='ranking_wrap')

        if not ranking_section:
            print("Ranking section (.ranking_wrap) not found")
            return []

        # ì¹´ë“œ í•­ëª©ë“¤ ì°¾ê¸° - li íƒœê·¸ë¡œ êµ¬ì„±ë¨
        card_items = ranking_section.find_all('li')

        if not card_items:
            # ëŒ€ì²´ ì„ íƒì ì‹œë„
            card_items = ranking_section.find_all('article') or \
                ranking_section.find_all(
                    'div', class_=re.compile('card.*item|item.*card', re.I))

        print(f"Found {len(card_items)} card items")

        for item in card_items:
            try:
                card_data = self._parse_card_item(item)
                if card_data:
                    # nameì´ ì—†ì–´ë„ ë‹¤ë¥¸ ì •ë³´ê°€ ìˆìœ¼ë©´ ì €ì¥
                    if card_data.get('name') or card_data.get('link'):
                        cards.append(card_data)
                        rank = card_data.get('rank', '?')
                        name = card_data.get('name', card_data.get(
                            'raw_link_text', 'Unknown'))
                        print(f"Parsed card {rank}: {name[:50]}")
                    else:
                        # ë””ë²„ê¹…: ì™œ íŒŒì‹±ì´ ì•ˆ ë˜ëŠ”ì§€ í™•ì¸
                        if len(cards) < 3:  # ì²˜ìŒ 3ê°œë§Œ ë””ë²„ê¹…
                            print(f"Debug: Skipped item - {card_data}")
            except Exception as e:
                print(f"Error parsing card item: {e}")
                import traceback
                if len(cards) < 3:  # ì²˜ìŒ 3ê°œ ì—ëŸ¬ë§Œ ìƒì„¸ ì¶œë ¥
                    traceback.print_exc()
                continue

        return cards

    def _parse_card_item(self, item) -> Dict:
        """
        ê°œë³„ ì¹´ë“œ í•­ëª© íŒŒì‹±
        ì‹¤ì œ HTML êµ¬ì¡°:
        - li íƒœê·¸ ë‚´ë¶€
        - .num: ìˆœìœ„ ë²ˆí˜¸
        - .updown: ìˆœìœ„ ë³€ë™ (up/down/default)
        - a[href*="/card/detail/"]: ì¹´ë“œ ìƒì„¸ ë§í¬ ë° ì¹´ë“œëª…
        - img: ì¹´ë“œ ì´ë¯¸ì§€
        """
        card_data = {
            'scraped_at': datetime.now().isoformat()
        }

        # ìˆœìœ„ ë²ˆí˜¸ (.num)
        num_elem = item.find('div', class_='num')
        if num_elem:
            rank_text = num_elem.get_text(strip=True)
            try:
                card_data['rank'] = int(rank_text)
            except ValueError:
                card_data['rank'] = rank_text

        # ìˆœìœ„ ë³€ë™ (.updown)
        updown_elem = item.find('div', class_='updown')
        if updown_elem:
            rank_change = updown_elem.get_text(strip=True)
            card_data['rank_change'] = rank_change

            # ìˆœìœ„ ë³€ë™ ë°©í–¥ (up/down/default)
            updown_classes = updown_elem.get('class', [])
            if 'up' in updown_classes:
                card_data['rank_change_direction'] = 'up'
            elif 'down' in updown_classes:
                card_data['rank_change_direction'] = 'down'
            else:
                card_data['rank_change_direction'] = 'default'

        # ì¹´ë“œ ìƒì„¸ ë§í¬ ë° ì¹´ë“œëª…
        # ì—¬ëŸ¬ ë§í¬ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, name_areaë¥¼ í¬í•¨í•œ ë§í¬ë¥¼ ì°¾ìŒ
        link_elem = None
        for link in item.find_all('a', href=re.compile(r'/card/detail/')):
            # name_areaë¥¼ í¬í•¨í•œ ë§í¬ê°€ ì‹¤ì œ ì¹´ë“œëª… ë§í¬
            if link.find('div', class_='name_area') or link.find('p', class_='card_name'):
                link_elem = link
                break

        # name_areaë¥¼ í¬í•¨í•œ ë§í¬ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë§í¬ ì‚¬ìš©
        if not link_elem:
            link_elem = item.find('a', href=re.compile(r'/card/detail/'))

        if link_elem:
            href = link_elem.get('href', '')
            if href.startswith('/'):
                card_data['link'] = self.base_url + href
            elif href.startswith('http'):
                card_data['link'] = href
            else:
                card_data['link'] = self.base_url + '/' + href

            # ì¹´ë“œëª… ì¶”ì¶œ - ì—¬ëŸ¬ ë°©ë²• ì‹œë„
            card_name = None

            # 1. p.card_nameì—ì„œ ì¶”ì¶œ
            card_name_elem = link_elem.find('p', class_='card_name')
            if card_name_elem:
                card_name = card_name_elem.get_text(strip=True)

            # 2. name_area ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
            if not card_name:
                name_area = link_elem.find('div', class_='name_area')
                if name_area:
                    # name_area ë‚´ë¶€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                    texts = [t.strip() for t in name_area.stripped_strings]
                    # ì¹´ë“œì‚¬ëª…ì´ ì•„ë‹Œ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì¹´ë“œëª…ìœ¼ë¡œ ì‚¬ìš©
                    if texts:
                        # ì¹´ë“œì‚¬ëª… ì œì™¸í•˜ê³  ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ì„ íƒ
                        card_issuers = ['ì‹ í•œì¹´ë“œ', 'ì‚¼ì„±ì¹´ë“œ', 'KBêµ­ë¯¼ì¹´ë“œ', 'í•˜ë‚˜ì¹´ë“œ', 'ë¡¯ë°ì¹´ë“œ',
                                        'í˜„ëŒ€ì¹´ë“œ', 'BCì¹´ë“œ', 'NHì¹´ë“œ', 'ìš°ë¦¬ì¹´ë“œ', 'IBKê¸°ì—…ì€í–‰ì¹´ë“œ',
                                        'ì¹´ì¹´ì˜¤ë±…í¬', 'í† ìŠ¤ì¹´ë“œ']
                        filtered_texts = [t for t in texts if not any(
                            issuer in t for issuer in card_issuers)]
                        if filtered_texts:
                            card_name = max(filtered_texts, key=len)
                        else:
                            card_name = max(texts, key=len)

            # 3. ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ (ì¹´ë“œì‚¬ëª… ì œì™¸)
            if not card_name:
                link_text = link_elem.get_text(strip=True)
                if link_text:
                    # ì¹´ë“œì‚¬ëª… ì œê±°
                    card_issuers = ['ì‹ í•œì¹´ë“œ', 'ì‚¼ì„±ì¹´ë“œ', 'KBêµ­ë¯¼ì¹´ë“œ', 'í•˜ë‚˜ì¹´ë“œ', 'ë¡¯ë°ì¹´ë“œ',
                                    'í˜„ëŒ€ì¹´ë“œ', 'BCì¹´ë“œ', 'NHì¹´ë“œ', 'ìš°ë¦¬ì¹´ë“œ', 'IBKê¸°ì—…ì€í–‰ì¹´ë“œ',
                                    'ì¹´ì¹´ì˜¤ë±…í¬', 'í† ìŠ¤ì¹´ë“œ']
                    cleaned_text = link_text
                    for issuer in card_issuers:
                        cleaned_text = cleaned_text.replace(issuer, '').strip()
                    if cleaned_text:
                        card_name = cleaned_text
                    else:
                        card_name = link_text

            # ì˜ëª»ëœ ì¹´ë“œëª… í•„í„°ë§ (í˜ì´ì§€ ì œëª© ë“±)
            invalid_names = ['ğŸ† ì‹ ìš©ì¹´ë“œ ì‹¤ì‹œê°„ ì¸ê¸°ìˆœìœ„', 'ì‹ ìš©ì¹´ë“œ ì‹¤ì‹œê°„ ì¸ê¸°ìˆœìœ„',
                             'ì¸ê¸°ìˆœìœ„', 'ì¹´ë“œê³ ë¦´ë¼', 'TOP100']
            if card_name:
                # ì˜ëª»ëœ ê°’ì´ë©´ Noneìœ¼ë¡œ ì„¤ì •
                if any(invalid in card_name for invalid in invalid_names):
                    card_name = None

            if card_name:
                card_data['name'] = card_name
            else:
                card_data['raw_link_text'] = link_elem.get_text(strip=True)

            # ì´ë²¤íŠ¸ í…ìŠ¤íŠ¸ (í˜œíƒ ì„¤ëª…)
            event_elem = link_elem.find('p', class_='event_txt')
            if event_elem:
                card_data['event_text'] = event_elem.get_text(strip=True)

            # ì¹´ë“œì‚¬ëª…
            corp_elem = link_elem.find('p', class_='corp_name')
            if corp_elem:
                card_data['issuer'] = corp_elem.get_text(strip=True)

        # ì¹´ë“œ ì´ë¯¸ì§€
        img_elem = item.find('img')
        if img_elem:
            src = img_elem.get('src') or img_elem.get('data-src', '')
            if src:
                if src.startswith('//'):
                    card_data['image'] = 'https:' + src
                elif src.startswith('/'):
                    card_data['image'] = self.base_url + src
                elif src.startswith('http'):
                    card_data['image'] = src
                else:
                    card_data['image'] = self.base_url + '/' + src

                # ì´ë¯¸ì§€ alt í…ìŠ¤íŠ¸
                if img_elem.get('alt'):
                    card_data['image_alt'] = img_elem.get('alt')
                    # ì¹´ë“œëª…ì´ ì—†ìœ¼ë©´ ì´ë¯¸ì§€ altì—ì„œ ì¶”ì¶œ ì‹œë„
                    if 'name' not in card_data or not card_data.get('name'):
                        alt_text = img_elem.get('alt', '').strip()
                        if alt_text and len(alt_text) > 2:
                            # ì¹´ë“œì‚¬ëª… ì œê±°
                            card_issuers = ['ì‹ í•œì¹´ë“œ', 'ì‚¼ì„±ì¹´ë“œ', 'KBêµ­ë¯¼ì¹´ë“œ', 'í•˜ë‚˜ì¹´ë“œ', 'ë¡¯ë°ì¹´ë“œ',
                                            'í˜„ëŒ€ì¹´ë“œ', 'BCì¹´ë“œ', 'NHì¹´ë“œ', 'ìš°ë¦¬ì¹´ë“œ', 'IBKê¸°ì—…ì€í–‰ì¹´ë“œ',
                                            'ì¹´ì¹´ì˜¤ë±…í¬', 'í† ìŠ¤ì¹´ë“œ']
                            cleaned_alt = alt_text
                            for issuer in card_issuers:
                                cleaned_alt = cleaned_alt.replace(
                                    issuer, '').strip()
                            if cleaned_alt:
                                card_data['name'] = cleaned_alt
                            else:
                                card_data['name'] = alt_text

        # ì¹´ë“œì‚¬ëª…ì´ ì•„ì§ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì‹œë„
        if 'issuer' not in card_data:
            full_text = item.get_text(strip=True)
            if full_text:
                # ì¹´ë“œì‚¬ëª… ì¶”ì¶œ ì‹œë„ (ì¼ë°˜ì ì¸ ì¹´ë“œì‚¬ëª… íŒ¨í„´)
                card_issuers = ['ì‹ í•œì¹´ë“œ', 'ì‚¼ì„±ì¹´ë“œ', 'KBêµ­ë¯¼ì¹´ë“œ', 'í•˜ë‚˜ì¹´ë“œ', 'ë¡¯ë°ì¹´ë“œ',
                                'í˜„ëŒ€ì¹´ë“œ', 'BCì¹´ë“œ', 'NHì¹´ë“œ', 'ìš°ë¦¬ì¹´ë“œ', 'IBKê¸°ì—…ì€í–‰ì¹´ë“œ',
                                'ì¹´ì¹´ì˜¤ë±…í¬', 'í† ìŠ¤ì¹´ë“œ']
                for issuer in card_issuers:
                    if issuer in full_text:
                        card_data['issuer'] = issuer
                        break

        return card_data

    def scrape_card_detail(self, card_url: str) -> Dict:
        """ê°œë³„ ì¹´ë“œ ìƒì„¸ì •ë³´ ìŠ¤í¬ë˜í•‘ (í…ìŠ¤íŠ¸ ì„¤ëª… í¬í•¨)"""
        if not self.use_playwright:
            print("âš ï¸  Playwrightê°€ í•„ìš”í•©ë‹ˆë‹¤. ìƒì„¸ì •ë³´ ìŠ¤í¬ë˜í•‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {}

        soup = self.get_page(card_url)

        if not soup:
            return {}

        detail = {
            'url': card_url,
            'scraped_at': datetime.now().isoformat()
        }

        try:
            # ì¹´ë“œëª… - ì—¬ëŸ¬ ë°©ë²• ì‹œë„
            card_name = None

            # 1. h1 íƒœê·¸ì—ì„œ ì¶”ì¶œ
            title = soup.find('h1')
            if title:
                card_name = title.get_text(strip=True)

            # 2. h2 íƒœê·¸ì—ì„œ ì¶”ì¶œ (title, name í´ë˜ìŠ¤)
            if not card_name:
                title = soup.find('h2', class_=re.compile('title|name', re.I))
                if title:
                    card_name = title.get_text(strip=True)

            # 3. card_name í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ìš”ì†Œì—ì„œ ì¶”ì¶œ
            if not card_name:
                name_elem = soup.find(class_=re.compile(
                    'card.*name|name.*card', re.I))
                if name_elem:
                    card_name = name_elem.get_text(strip=True)

            # 4. ìƒì„¸ í˜ì´ì§€ì˜ ì œëª© ì˜ì—­ì—ì„œ ì¶”ì¶œ
            if not card_name:
                title_section = soup.find(
                    'div', class_=re.compile('title|header|name', re.I))
                if title_section:
                    # h1, h2, strong íƒœê·¸ ì°¾ê¸°
                    title_tag = title_section.find(['h1', 'h2', 'strong'])
                    if title_tag:
                        card_name = title_tag.get_text(strip=True)
                    else:
                        # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ë…¸ë“œ ì‚¬ìš©
                        text = title_section.get_text(strip=True)
                        if text:
                            # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ì²« ë²ˆì§¸ ì¤„ ì‚¬ìš©
                            lines = [line.strip()
                                     for line in text.split('\n') if line.strip()]
                            if lines:
                                card_name = lines[0]

            # 5. í˜ì´ì§€ íƒ€ì´í‹€ì—ì„œ ì¶”ì¶œ (fallback)
            if not card_name:
                page_title = soup.find('title')
                if page_title:
                    title_text = page_title.get_text(strip=True)
                    # "ì¹´ë“œê³ ë¦´ë¼" ê°™ì€ ì‚¬ì´íŠ¸ëª… ì œê±°
                    if 'ì¹´ë“œê³ ë¦´ë¼' in title_text:
                        parts = title_text.split('ì¹´ë“œê³ ë¦´ë¼')
                        if parts:
                            card_name = parts[0].strip()
                    else:
                        card_name = title_text

            if card_name:
                detail['name'] = card_name

            # ì¹´ë“œì‚¬
            issuer = soup.find(
                'span', class_=re.compile('issuer|company', re.I))
            if issuer:
                detail['issuer'] = issuer.get_text(strip=True)

            # ì—°íšŒë¹„ ì •ë³´
            fee_section = soup.find('dl', class_=re.compile('fee|annual', re.I)) or \
                soup.find('div', class_=re.compile('fee|annual', re.I))
            if fee_section:
                detail['annual_fee'] = self._parse_fee_section(fee_section)

            # í˜œíƒ ì •ë³´
            benefit_section = soup.find('div', class_=re.compile('benefit', re.I)) or \
                soup.find('section', class_=re.compile('benefit', re.I))
            if benefit_section:
                detail['benefits'] = self._parse_benefits(benefit_section)

            # ì¹´ë“œ ìŠ¤í™
            spec_table = soup.find('table') or soup.find(
                'dl', class_=re.compile('spec|info', re.I))
            if spec_table:
                detail['specifications'] = self._parse_specifications(
                    spec_table)

            # ì¹´ë“œ ì„¤ëª… í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            detail['description_text'] = self._extract_description_text(soup)

        except Exception as e:
            print(f"Error parsing card detail: {e}")

        return detail

    def _extract_description_text(self, soup: BeautifulSoup) -> Dict:
        """ì¹´ë“œ ì„¤ëª… í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì£¼ìš”í˜œíƒ, ìœ ì˜ì‚¬í•­, ì—°ê´€ ì½˜í…ì¸ )"""
        description = {
            'benefits_text': [],
            'notices_text': [],
            'related_articles': []
        }

        try:
            # ì£¼ìš”í˜œíƒ ì„¹ì…˜ ì°¾ê¸°
            benefit_heading = soup.find('h3', string=re.compile('ì£¼ìš”í˜œíƒ'))
            if benefit_heading:
                benefit_article = benefit_heading.find_parent('article')
                if benefit_article:
                    # dt, dd êµ¬ì¡°ë¡œ ëœ í˜œíƒ ì„¤ëª…ë“¤
                    benefit_items = benefit_article.find_all(['dt', 'dd'])
                    for item in benefit_items:
                        text = item.get_text(strip=True)
                        if text and len(text) > 5 and text not in description['benefits_text']:
                            description['benefits_text'].append(text)

                    # ìœ ì˜ì‚¬í•­ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                    paragraphs = benefit_article.find_all('p')
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if text and (text.startswith('â€»') or text.startswith('Â·') or
                                     text.startswith('*') or 'ìœ ì˜' in text or 'ì£¼ì˜' in text):
                            if text not in description['notices_text']:
                                description['notices_text'].append(text)

                    # í•˜ë‹¨ ì•ˆë‚´ í…ìŠ¤íŠ¸
                    notice_divs = benefit_article.find_all(
                        'div', class_=re.compile('notice|caution|warning', re.I))
                    for div in notice_divs:
                        text = div.get_text(strip=True)
                        if text and text not in description['notices_text']:
                            description['notices_text'].append(text)

            # ì—°ê´€ ì½˜í…ì¸  ì„¹ì…˜ ì°¾ê¸°
            related_heading = soup.find('h3', string=re.compile('ì—°ê´€'))
            if related_heading:
                related_article = related_heading.find_parent('article')
                if related_article:
                    # ì—°ê´€ ì½˜í…ì¸  ë§í¬ë“¤
                    related_links = related_article.find_all(
                        'a', href=re.compile(r'/contents/'))
                    for link in related_links[:5]:  # ìµœëŒ€ 5ê°œë§Œ
                        title_elem = link.find('p')
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                            # ì„¤ëª… í…ìŠ¤íŠ¸ ì°¾ê¸°
                            desc_elems = link.find_all(
                                'p')[1:]  # ì²« ë²ˆì§¸ pëŠ” ì œëª©ì´ë¯€ë¡œ ì œì™¸
                            description_text = ' '.join(
                                [p.get_text(strip=True) for p in desc_elems[:2]])

                            if title:
                                description['related_articles'].append({
                                    'title': title,
                                    'description': description_text[:500] if description_text else ''
                                })

            # ì „ì²´ ì„¤ëª… í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° (ê²€ìƒ‰/ìš”ì•½ìš©)
            all_text_parts = []
            all_text_parts.extend(description['benefits_text'])
            all_text_parts.extend(description['notices_text'])
            all_text_parts.extend([art.get('description', '')
                                  for art in description['related_articles']])

            description['full_description'] = ' '.join(all_text_parts)

        except Exception as e:
            print(f"Error extracting description text: {e}")

        return description

    def _parse_fee_section(self, section) -> Dict:
        """ì—°íšŒë¹„ ì„¹ì…˜ íŒŒì‹±"""
        fees = {}
        try:
            # dt, dd êµ¬ì¡°
            dts = section.find_all('dt')
            dds = section.find_all('dd')
            for dt, dd in zip(dts, dds):
                key = dt.get_text(strip=True)
                value = dd.get_text(strip=True)
                fees[key] = value
        except:
            fees['raw'] = section.get_text(strip=True)
        return fees

    def _parse_benefits(self, section) -> List[Dict]:
        """í˜œíƒ ì„¹ì…˜ íŒŒì‹±"""
        benefits = []
        try:
            # li í•­ëª©ë“¤
            items = section.find_all('li') or section.find_all(
                'div', class_=re.compile('item', re.I))
            for item in items:
                benefit = {}

                # ì¹´í…Œê³ ë¦¬
                category = item.find(
                    'span', class_=re.compile('category|type', re.I))
                if category:
                    benefit['category'] = category.get_text(strip=True)

                # í• ì¸ìœ¨/í˜œíƒ
                discount = item.find('span', class_=re.compile('discount|rate|percent', re.I)) or \
                    item.find('strong')
                if discount:
                    benefit['discount'] = discount.get_text(strip=True)

                # ì„¤ëª…
                desc = item.find('p') or item
                if desc:
                    benefit['description'] = desc.get_text(strip=True)

                if benefit:
                    benefits.append(benefit)
        except Exception as e:
            print(f"Error parsing benefits: {e}")
        return benefits

    def _parse_specifications(self, table) -> Dict:
        """ìŠ¤í™ í…Œì´ë¸” íŒŒì‹±"""
        specs = {}
        try:
            if table.name == 'table':
                rows = table.find_all('tr')
                for row in rows:
                    th = row.find('th')
                    td = row.find('td')
                    if th and td:
                        specs[th.get_text(strip=True)] = td.get_text(
                            strip=True)
            else:  # dl êµ¬ì¡°
                dts = table.find_all('dt')
                dds = table.find_all('dd')
                for dt, dd in zip(dts, dds):
                    specs[dt.get_text(strip=True)] = dd.get_text(strip=True)
        except Exception as e:
            print(f"Error parsing specs: {e}")
        return specs

    def save_to_json(self, data: List[Dict], filename: str):
        """ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… Saved {len(data)} items to {filename}")

    def save_to_csv(self, data: List[Dict], filename: str):
        """ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not data:
            print("No data to save")
            return

        # benefits ê°™ì€ ë¦¬ìŠ¤íŠ¸ í•„ë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        flattened_data = []
        for item in data:
            flat_item = item.copy()
            for key, value in flat_item.items():
                if isinstance(value, (list, dict)):
                    flat_item[key] = json.dumps(value, ensure_ascii=False)
            flattened_data.append(flat_item)

        keys = flattened_data[0].keys()
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            writer.writerows(flattened_data)
        print(f"âœ… Saved {len(data)} items to {filename}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = CardGorillaScraper()

    print("=" * 60)
    print("ì¹´ë“œê³ ë¦´ë¼ TOP100 ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    print("=" * 60)

    # 1. TOP100 ì¸ê¸° ì¹´ë“œ ìŠ¤í¬ë˜í•‘ (ì£¼ê°„ ê¸°ì¤€)
    print("\n[1/2] TOP100 ì¹´ë“œ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì¤‘... (ì£¼ê°„ ê¸°ì¤€)")
    top100_cards = scraper.scrape_top100_cards(term='weekly')

    if top100_cards:
        print(f"\nâœ… ì´ {len(top100_cards)}ê°œì˜ ì¹´ë“œ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

        # JSON ì €ì¥
        scraper.save_to_json(top100_cards, 'cardgorilla_top100.json')

        # CSV ì €ì¥
        scraper.save_to_csv(top100_cards, 'cardgorilla_top100.csv')

        # ìƒ˜í”Œ ì¶œë ¥
        print("\n[ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°]")
        print("-" * 60)
        for card in top100_cards[:3]:
            print(f"ìˆœìœ„: {card.get('rank')}")
            print(f"ì¹´ë“œëª…: {card.get('name')}")
            print(f"ì¹´ë“œì‚¬: {card.get('issuer')}")
            print(f"ì—°íšŒë¹„: {card.get('annual_fee')}")
            print(f"ë§í¬: {card.get('link')}")
            print("-" * 60)

        # 2. TOP100 ì „ì²´ ì¹´ë“œ ìƒì„¸ì •ë³´ ë° í…ìŠ¤íŠ¸ ìŠ¤í¬ë˜í•‘
        print(f"\n[2/2] TOP100 ì „ì²´ ì¹´ë“œ ìƒì„¸ì •ë³´ ë° í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘...")
        print(f"ì´ {len(top100_cards)}ê°œ ì¹´ë“œì˜ ìƒì„¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. (ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        detailed_cards = []
        failed_cards = []

        for idx, card in enumerate(top100_cards, 1):
            if 'link' in card and card.get('link'):
                # ì¹´ë“œëª… ì¶”ì¶œ (ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ì‹œë„)
                card_name = card.get('name') or card.get(
                    'image_alt') or card.get('raw_link_text') or f"ì¹´ë“œ #{idx}"
                print(
                    f"  [{idx}/{len(top100_cards)}] {card_name} ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì¤‘...", end=' ', flush=True)
                try:
                    detail = scraper.scrape_card_detail(card['link'])
                    if detail:
                        # ê¸°ë³¸ ì •ë³´ì™€ ìƒì„¸ì •ë³´ ë³‘í•©
                        merged = {**card, **detail}

                        # ì¹´ë“œëª… ìš°ì„ ìˆœìœ„: card (ì˜¬ë°”ë¥¸ ê°’) > detail > fallback
                        # ì˜ëª»ëœ ê°’ í•„í„°ë§
                        invalid_names = ['ğŸ† ì‹ ìš©ì¹´ë“œ ì‹¤ì‹œê°„ ì¸ê¸°ìˆœìœ„', 'ì‹ ìš©ì¹´ë“œ ì‹¤ì‹œê°„ ì¸ê¸°ìˆœìœ„',
                                         'ì¸ê¸°ìˆœìœ„', 'ì¹´ë“œê³ ë¦´ë¼', 'TOP100']

                        # 1. ê¸°ë³¸ ì •ë³´ì˜ ì¹´ë“œëª…ì´ ì˜¬ë°”ë¥¸ ê°’ì´ë©´ ìš°ì„  ì‚¬ìš©
                        card_name = card.get('name', '').strip()
                        if card_name and not any(invalid in card_name for invalid in invalid_names):
                            merged['name'] = card_name
                        # 2. ìƒì„¸ì •ë³´ì—ì„œ ì¶”ì¶œí•œ ì¹´ë“œëª…ì´ ìˆìœ¼ë©´ ì‚¬ìš©
                        elif detail.get('name') and detail.get('name').strip():
                            detail_name = detail.get('name').strip()
                            # ìƒì„¸ì •ë³´ì˜ nameë„ ì˜ëª»ëœ ê°’ì´ ì•„ë‹Œì§€ í™•ì¸
                            if not any(invalid in detail_name for invalid in invalid_names):
                                merged['name'] = detail_name
                            else:
                                # ìƒì„¸ì •ë³´ë„ ì˜ëª»ëœ ê°’ì´ë©´ URLì—ì„œ ì¶”ì¶œ
                                url_parts = card.get('link', '').split('/')
                                if url_parts:
                                    merged['name'] = f"ì¹´ë“œ {url_parts[-1]}"
                        # 3. ë‘˜ ë‹¤ ì—†ê±°ë‚˜ ëª¨ë‘ ì˜ëª»ëœ ê°’ì´ë©´ URLì—ì„œ ì¶”ì¶œ
                        else:
                            url_parts = card.get('link', '').split('/')
                            if url_parts:
                                merged['name'] = f"ì¹´ë“œ {url_parts[-1]}"

                        detailed_cards.append(merged)
                        # ì„¤ëª… í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                        has_text = 'description_text' in detail and detail['description_text'].get(
                            'full_description')
                        if has_text:
                            text_len = len(detail['description_text'].get(
                                'full_description', ''))
                            print(f"âœ… (í…ìŠ¤íŠ¸: {text_len}ì)")
                        else:
                            print("âœ… (í…ìŠ¤íŠ¸ ì—†ìŒ)")
                    else:
                        print("âš ï¸  (ìƒì„¸ì •ë³´ ì—†ìŒ)")
                        # ìƒì„¸ì •ë³´ê°€ ì—†ì–´ë„ ê¸°ë³¸ ì •ë³´ëŠ” ì €ì¥
                        detailed_cards.append(card)
                except Exception as e:
                    print(f"âŒ (ì—ëŸ¬: {str(e)[:50]})")
                    failed_cards.append(
                        {'card': card_name or f"ì¹´ë“œ #{idx}", 'error': str(e)})
                    # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ì •ë³´ëŠ” ì €ì¥
                    detailed_cards.append(card)
            else:
                # ë§í¬ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥
                detailed_cards.append(card)

        if detailed_cards:
            scraper.save_to_json(
                detailed_cards, 'cardgorilla_top100_detailed.json')
            print(f"\nâœ… ìƒì„¸ì •ë³´ {len(detailed_cards)}ê°œ ì €ì¥ ì™„ë£Œ")

            # ì„¤ëª… í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ì¹´ë“œ ìˆ˜ í™•ì¸
            cards_with_text = sum(1 for card in detailed_cards
                                  if card.get('description_text') and
                                  card.get('description_text', {}).get('full_description'))
            print(f"   - ì„¤ëª… í…ìŠ¤íŠ¸ í¬í•¨: {cards_with_text}ê°œ")

            if failed_cards:
                print(f"\nâš ï¸  {len(failed_cards)}ê°œ ì¹´ë“œì—ì„œ ì—ëŸ¬ ë°œìƒ:")
                for failed in failed_cards[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                    print(f"   - {failed['card']}: {failed['error'][:50]}")

    else:
        print("\nâŒ ì¹´ë“œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("HTML êµ¬ì¡°ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")

    print("\n" + "=" * 60)
    print("ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ì¹´ë“œê³ ë¦´ë¼ ì›¹ ìŠ¤í¬ë˜í¼ v2.0                        â•‘
â•‘         ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âš ï¸  ì£¼ì˜ì‚¬í•­:
1. ì´ ë„êµ¬ëŠ” ê°œì¸ì ì¸ ì—°êµ¬/í•™ìŠµ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ê³¼ë„í•œ ìš”ì²­ì€ ì„œë²„ì— ë¶€ë‹´ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
3. ìˆ˜ì§‘í•œ ë°ì´í„°ì˜ ìƒì—…ì  ì‚¬ìš©ì€ ì €ì‘ê¶Œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤
4. robots.txt ë° ì´ìš©ì•½ê´€ì„ í™•ì¸í•˜ì„¸ìš”

ì‹œì‘í•˜ë ¤ë©´ main() í•¨ìˆ˜ì˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
    """)

    # ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
    main()
